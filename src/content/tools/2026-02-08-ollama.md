---
title: "Ollama"
description: "Run open-source LLMs locally with one command. No GPU required."
url: "https://ollama.com"
status: active
tags: [local-llm, open-source, self-hosted]
draft: false
---

Ollama makes running local LLMs dead simple. `ollama run llama3` and you've got a chatbot running on your own hardware. No API keys, no cloud, no data leaving your machine.

The model library is solid. Llama 3, Mistral, Gemma, Code Llama, Phi. All downloadable and runnable in one command. It handles the quantisation and memory management so you don't have to.

Performance depends on your hardware. On a decent CPU you'll get usable speeds with smaller models. A GPU obviously helps but isn't required. I've run 7B models on modest hardware and they're surprisingly capable for local work.

Good for prototyping, experimentation, and anything where you don't want to send data to an API.
